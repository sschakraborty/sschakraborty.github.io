<html>
    <head>
        <title>Sorting Tutorials</title>
        <link rel="stylesheet" href="../../vendor/bootstrap/css/bootstrap.min.css" />
        <link rel="stylesheet" href="../../vendor/font-awesome/css/font-awesome.min.css" />
        <link rel="stylesheet" href="../../vendor/prism/prism.css" />
        
        <script type="text/javascript" src="../../vendor/jquery/jquery.min.js"></script>
        <script type="text/javascript" src="../../vendor/bootstrap/js/bootstrap.min.js"></script>
        <script type="text/javascript" src="../../vendor/prism/prism.js"></script>
    </head>
    <body>
        <nav class="navbar navbar-light bg-light bg-red border-dark border-bottom fixed-top">
            <a class="navbar-brand" href="#">Sorting</a>
            
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav mr-auto border border-dark rounded text-center" style="margin: 25px !important; padding: 25px !important;">
                    <li class="nav-item"><a class="nav-link" href="../../">Website Main Page</a></li>
                    <li class="nav-item"><a class="nav-link" href="../">Tutorials Home</a></li>
                </ul>
            </div>
        </nav>
        
        <div class="container" style="margin-top: 70px !important;">
            <h2>Theory of Sorting</h2>
            <br>
            In computer science, arranging in an ordered sequence is called "sorting". Sorting is a common operation in many applications, and efficient algorithms to perform it have been developed.
            <br>
            The most common uses of sorted sequences are:
            <br>
            <ul>
                <li>Making lookup or search efficient.</li>
                <li>Making merging of sequences efficient.</li>
                <li>Enable processing of data in a defined order.</li>
            </ul>

            The opposite of sorting, rearranging a sequence of items in a random or meaningless order, is called shuffling.
            <br><br>

            For sorting, either a weak order, "should not come after", can be specified, or a strict weak order, "should come before" (specifying one defines also the other, the two are the complement of the inverse of each other, see operations on binary relations). For the sorting to be unique, these two are restricted to a total order and a strict total order, respectively.
            <br><br>

            Sorting n-tuples (depending on context also called e.g. records consisting of fields) can be done based on one or more of its components. More generally objects can be sorted based on a property. Such a component or property is called a sort key.
            <br><br>

            For example, the items are books, the sort key is the title, subject or author, and the order is alphabetical.
            <br><br>

            A new sort key can be created from two or more sort keys by lexicographical order. The first is then called the primary sort key, the second the secondary sort key, etc.
            <br><br>

            For example, addresses could be sorted using the city as primary sort key, and the street as secondary sort key.
            <br><br>

            If the sort key values are totally ordered, the sort key defines a weak order of the items: items with the same sort key are equivalent with respect to sorting. See also stable sorting. If different items have different sort key values then this defines a unique order of the items.
            <br><br>

            Workers sorting parcels in a postal facility
            <br><br>

            A standard order is often called ascending (corresponding to the fact that the standard order of numbers is ascending, i.e. A to Z, 0 to 9), the reverse order descending (Z to A, 9 to 0). For dates and times, ascending means that earlier values precede later ones e.g. 1/1/2000 will sort ahead of 1/1/2001.
            
            <br><br>
            <h2>Stable v/s Unstable Sorting</h2>
            <br>
            
            A sorting algorithm is said to be stable if two objects with equal keys appear in the same order in sorted output as they appear in the input unsorted array.
            <br>
            Whereas a sorting algorithm is said to be unstable if their are two or more objects with equal keys doesn't appear in same order before and after sorting.
            
            <br><br>
            <div style="text-align: center;">
                <img src="stable_unstable.jpeg">
            </div>
            
            <br><br>
            <h2>Stable v/s Unstable Sorting</h2>
            <br>
            
            A sorting algorithm using at most <code>h</code> comparisons on all inputs corresponds to a tree of height at most <code>h</code>. Such a tree has at most <code>2 &times; h</code> leaves. On the other hand, each permutation of <code>1, ... ,N</code> must land at a different leaf, and so there must be at least <code>N!</code> leaves. Putting these together, we deduce that <code>2 &times; h &ge; N!</code> and so <code>h &ge; log<sub>2</sub> N! = &Omega;(N log N)</code> (using Stirling's approximation). So every sorting algorithm must use at least <code>log<sub>2</sub> N! = &Omega;(N log N)</code> comparisons in the worst case (on some inputs it can use less)
            
        </div>
        
        <div class="container" style="margin-top: 70px !important;">
            <h2>Selection Sort</h2>
            <h3>(Stable comparison sort of <code>&theta;(N<sup>2</sup>)</code> complexity)</h3>
            <br>
            
            Selection sort is a sorting algorithm, specifically an in-place comparison sort. It has <code>&theta;(N<sup>2</sup>)</code> time complexity, making it inefficient on large lists, and generally performs worse than the similar insertion sort. Selection sort is noted for its simplicity, and it has performance advantages over more complicated algorithms in certain situations, particularly where auxiliary memory is limited.
            <br><br>
            The algorithm divides the input list into two parts: the sublist of items already sorted, which is built up from left to right at the front (left) of the list, and the sublist of items remaining to be sorted that occupy the rest of the list. Initially, the sorted sublist is empty and the unsorted sublist is the entire input list. The algorithm proceeds by finding the smallest (or largest, depending on sorting order) element in the unsorted sublist, exchanging (swapping) it with the leftmost unsorted element (putting it in sorted order), and moving the sublist boundaries one element to the right.
            <br>
            <br>
            Points to note:
            <ul>
                <li>Selection sort is an in-place comparison sort.
                <li>Best, average and worst case time complexity is <code>&theta;(N<sup>2</sup>)</code>
                <li>It has one of the lowest number of swaps of all sorting algorithms.
            </ul>
            
            <img src="selection_sort.png" style="height: 500px; width: 100%;">
            
            <br>
            <pre class="lang-python"><code># Selection sort implementation in Python 3

from random import random as r

array = [int(r() * 1024) for i in range(16)]


def sort():
    for i in range(len(array)):
        min_value = array[i]
        min_pos = i
        for j in range(i + 1, len(array)):
            if array[j] < min_value:
                min_value = array[j]
                min_pos = j
        # Swap array[i] with array[min_pos]
        array[i], array[min_pos] = array[min_pos], array[i]


if __name__ == "__main__":
    print("Unsorted Array: ", array)
    sort()
    print("Sorted Array: ", array)</code></pre>
        </div>
        
        
        <div class="container" style="margin-top: 70px !important;">
            <h2>Bubble Sort</h2>
            <h3>(Stable comparison sort of <code>&theta;(N<sup>2</sup>)</code> complexity)</h3>
            <br>
            
            Bubble sort, sometimes referred to as <code>sinking sort</code>, is a simple sorting algorithm that repeatedly steps through the list to be sorted, compares each pair of adjacent items and swaps them if they are in the wrong order. The pass through the list is repeated until no swaps are needed, which indicates that the list is sorted. The algorithm, which is a comparison sort, is named for the way smaller or larger elements "bubble" to the top of the list. Although the algorithm is simple, it is too slow and impractical for most problems even when compared to insertion sort. It can be practical if the input is usually in sorted order but may occasionally have some out-of-order elements nearly in position.
            
            Bubble sort has worst-case and average complexity both <code>&theta;(N<sup>2</sup>)</code>, where n is the number of items being sorted. There exist many sorting algorithms, such as merge sort with substantially better worst-case or average complexity of <code>&theta;(N log<sub>2</sub> N)</code>. Even other <code>&theta;(N<sup>2</sup>)</code> sorting algorithms, such as insertion sort, tend to have better performance than bubble sort. Therefore, bubble sort is not a practical sorting algorithm when n is large.
            
            <div style="text-align: center;">
                <img src="bubble_sort.png" style="height: 500px; width: 650px;">
            </div>
            
            <pre class="lang-python"><code># Bubble Sort implementation in Python 3

from random import random as r

array = [int(r() * 1024) for i in range(16)]


def sort():
    for i in range(len(array)):
        for j in range(len(array) - i - 1):
            if array[j] > array[j + 1]:
                array[j], array[j + 1] = array[j + 1], array[j]


if __name__ == "__main__":
    print("Unsorted Array: ", array)
    sort()
    print("Sorted Array: ", array)</code></pre>
        </div>
        
        
        <div class="container" style="margin-top: 70px !important;">
            <h2>Quick Sort</h2>
            <h3>(Comparison sort of <code>&theta;(N log<sub>2</sub> N)</code> average-case complexity</h3>
            <h3>Has <code>&theta;(N<sup>2</sup>)</code> worst-case complexity)</h3>
            <br>
            
            Quick Sort (sometimes called partition-exchange sort) is an efficient sorting algorithm, serving as a systematic method for placing the elements of an array in order. Developed by Tony Hoare in 1959 and published in 1961, it is still a commonly used algorithm for sorting. When implemented well, it can be about two or three times faster than it's main competitors, merge sort and heapsort.
            <br><br>
            Quick Sort is a comparison sort, meaning that it can sort items of any type for which a "less-than" relation (formally, a total order) is defined. In efficient implementations it is not a stable sort, meaning that the relative order of equal sort items is not preserved. Quicksort can operate in-place on an array, requiring small additional amounts of memory to perform the sorting. It is very similar to selection sort, except that it does not always choose worst-case partition.
            <br><br>
            Mathematical analysis of quicksort shows that, on average, the algorithm takes <code>&theta;(N log<sub>2</sub> N)</code> comparisons to sort n items. In the worst case, it makes <code>O(N<sup>2</sup>)</code> comparisons, though this behavior is rare.
            
            <br><br>
            Example of partition function (Showing a single pass)
            <br>
            <div style="text-align: center;">
                <img src="quick_sort_1.png">
            </div>
            
            <br><br>
            Showing Quick Sort as divide-&amp;-conquer algorithm
            <br><br>
            <div style="text-align: center;">
                <img src="quick_sort_2.jpeg">
            </div>
            <br>
            
            <pre class="lang-python"><code># Quick Sort implementation in Python 3

from random import random as r

array = [int(r() * 1024) for i in range(16)]


# Lomuto Partition function
def partition(lb, ub):
    pivot_index = ub
    k = lb - 1
    for i in range(lb, ub):
        if array[i] < array[pivot_index]:
            k += 1
            array[k], array[i] = array[i], array[k]
    k += 1
    array[k], array[pivot_index] = array[pivot_index], array[k]
    return k


# Quick Sort recursive implementation
def quick_sort(lb, ub):
    if lb < ub:
        p = partition(lb, ub)
        quick_sort(lb, p - 1)
        quick_sort(p + 1, ub)


# Quick Sort implementation
def sort():
    quick_sort(0, len(array) - 1)


if __name__ == "__main__":
    print("Unsorted Array: ", array)
    sort()
    print("Sorted Array: ", array)</code></pre>
        </div>
        
        
        <div class="container" style="margin-top: 70px !important;">
            <h2>Heap Sort</h2>
            <h3>(Unstable comparison sort of <code>&theta;(N log<sub>2</sub> N)</code> complexity)</h3>
            <br>
            
            In computer science, heapsort is a comparison-based sorting algorithm. Heap Sort can be thought of as an improved selection sort: like that algorithm, it divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element and moving that to the sorted region. The improvement consists of the use of a heap data structure rather than a linear-time search to find the maximum.
            <br>
            <br>

            Although somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more
            favorable worst-case <code>&theta;(N log<sub>2</sub> N)</code> runtime. Heapsort is an in-place algorithm, but it is not a stable sort.
            <br>
            <br>

            Heap Sort was invented by J. W. J. Williams in 1964. This was also the birth of the heap, presented already by Williams as a useful data structure in its own right. In the same year, R. W. Floyd published an improved version that could sort an array in-place, continuing his earlier research into the treesort algorithm.
            
            <br><br>
            The heapsort algorithm can be divided into two parts.

            <br><br>
            In the first step, a heap is built out of the data. The heap is often placed in an array with the layout of a complete binary tree. The complete binary tree maps the binary tree structure into the array indices; each array index represents a node; the index of the node's parent, left child branch, or right child branch are simple expressions. For a zero-based array, the root node is stored at index 0; if i is the index of the current node, then

            <br><br>
            <code>
                iParent(i) = floor((i - 1) / 2) where floor functions map a real number to the smallest leading integer
                <br>
                iLeftChild(i)  = 2 &times; i + 1
                <br>
                iRightChild(i) = 2 &times; i + 2
            </code>
            
            <br><br>
            In the second step, a sorted array is created by repeatedly removing the largest element from the heap (the root of the heap), and inserting it into the array. The heap is updated after each removal to maintain the heap property. Once all objects have been removed from the heap, the result is a sorted array.

            <br><br>
            Heap Sort can be performed in place. The array can be split into two parts, the sorted array and the heap. The storage of heaps as arrays is diagrammed here. The heap's invariant is preserved after each extraction, so the only cost is that of extraction.
            
            <br><br>
            
            <div style="text-align: center;">
                <img src="heap_sort.png">
            </div>
            
            <br>
            <pre class="lang-python"><code># Heap Sort implementation in Python 3

from random import random as r

array = [int(r() * 1024) for i in range(16)]


def parent(index):
    return (index - 1) >> 1


def right(index):
    return (index << 1) + 1


def left(index):
    return (index << 1) + 2


# Function to build max heap from given array
def build_max_heap():
    for i in range(len(array)):
        index = i
        while parent(index) >= 0 and array[index] > array[parent(index)]:
            array[index], array[parent(index)] = array[parent(index)], array[index]
            index = parent(index)


# Iterative Heap Sort implementation
def sort():
    build_max_heap()
    size = len(array)
    for i in range(len(array)):
        array[0], array[size - 1] = array[size - 1], array[0]
        size -= 1
        # Swim down the top of heap until balanced
        current_index = 0
        while True:
            max_index = current_index
            if left(current_index) < size and array[left(current_index)] > array[max_index]:
                max_index = left(current_index)
            if right(current_index) < size and array[right(current_index)] > array[max_index]:
                max_index = right(current_index)
            if max_index != current_index:
                array[max_index], array[current_index] = array[current_index], array[max_index]
                current_index = max_index
            else:
                break


if __name__ == "__main__":
    print("Unsorted Array: ", array)
    build_max_heap()
    print("Max Heap: ", array)
    sort()
    print("Sorted Array: ", array)</code></pre>
        </div>
        
        <br>
    </body>
</html>
